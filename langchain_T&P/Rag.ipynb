{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03a3fa2",
   "metadata": {},
   "source": [
    "# Langchain :\n",
    "## 6. RAG :\n",
    "#### - Combining search + generation\n",
    "### why RAG :\n",
    "#### - Limitation of standalone LLM\n",
    "#### - Need for external knowledge\n",
    "\n",
    "| Chatgpt and other models                        | RAG                                                 |\n",
    "|-------------------------------------------------|-----------------------------------------------------|\n",
    "| Pure generation                                 | Retrival + Generation                               |\n",
    "\n",
    "### Components of RAG :\n",
    "#### 1. REtriver\n",
    "#### 2. Generator\n",
    "#### 4. Knowledge\n",
    "\n",
    "### Flow:\n",
    "#### input query -> Retriver -> Generator -> Final output\n",
    "\n",
    "![Embedding](images/RAG.png)\n",
    "\n",
    "### Types : \n",
    "#### 1. Self RAG\n",
    "#### 2. Corrective RAG \n",
    "#### 3. Fusion RAG\n",
    "#### 4. Advance RAG\n",
    "#### 5. Speculation RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de965ac2",
   "metadata": {},
   "source": [
    "### 1. Self RAG:\n",
    "#### - It checks the own answer, if unsure, retrives more information and tries again\n",
    "![Self Rag](images/self_RAG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_pinecone import Pinecone as LangchainPinecone\n",
    "\n",
    "# Step 1: Set your Gemini API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"xxxxxxxxxxxxxxxxxxxx\"  # ðŸ”’ Replace with your key\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"xxxxxxxxxxxxxxxxxxx\"  # ðŸ”’ Replace with your key\n",
    "\n",
    "# Step 2: Initialize the Gemini Embedding Model\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\"  # or use 'text-embedding-3-large' if needed\n",
    ")\n",
    "\n",
    "# Step 3: Connect to existing Pinecone vector store\n",
    "vectorstore = LangchainPinecone(\n",
    "    embedding=embedding_model,\n",
    "    index_name=\"langchainpdf\",  # âœ… Your existing index name\n",
    "    namespace=\"llmpdf\"          # âœ… The namespace you used earlier\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af3e0291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result #1:\n",
      "What is LangChain?\n",
      "\n",
      "Result #2:\n",
      "How does LangChain work?\n",
      "\n",
      "Result #3:\n",
      "LangChain comes with many extensions and a larger ecosystem that is developing around it.\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Run a test query (optional)\n",
    "query = \"What is LangChain?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "# STEP 5: Show results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nResult #{i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ee056",
   "metadata": {},
   "source": [
    "#### Adding Self Rag :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2ec16b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_pinecone import Pinecone as LangchainPinecone\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4083f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90e0f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 1: Initialize Gemini 2.5 Pro LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0.3,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "\n",
    "# âœ… Step 2: Connect LLM with your existing retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,  # Use your Pinecone or FAISS retriever here\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2566119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag_query(query):\n",
    "    \"\"\"\n",
    "    Perform a self-RAG query using the initialized LLM and retriever.\n",
    "    \"\"\"\n",
    "    print(\"First attempt without retriever:\")\n",
    "    \n",
    "    # âœ… Use invoke instead of predict\n",
    "    first_response_msg = llm.invoke(f\"Q: {query}\\nA: Please provide a detailed answer based on your knowledge.\")\n",
    "    first_response = first_response_msg.content\n",
    "\n",
    "    if (\n",
    "        \"I'm not sure\" in first_response\n",
    "        or \"I don't know\" in first_response\n",
    "        or len(first_response) < 30\n",
    "    ):\n",
    "        print(\"Low confidence response detected. Using RAG for better results...\")\n",
    "        improved_response = qa_chain.invoke({\"query\": query})  # Can also use qa_chain.run(query)\n",
    "        return improved_response\n",
    "    else:\n",
    "        return {\n",
    "            \"result\": first_response,\n",
    "            \"source_documents\": []\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "995961b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\social_Eagle_python\\siva\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First attempt without retriever:\n",
      "Low confidence response detected. Using RAG for better results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\social_Eagle_python\\siva\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response:\n",
      " Based on the provided context, I cannot explain the Transformer architecture in detail. The text fragments are incomplete and do not contain enough information for a detailed explanation.\n",
      "\n",
      "From the context, I can only tell you that:\n",
      "\n",
      "*   A Transformer is a Deep Learning (DL) architecture.\n",
      "*   It was first introduced in 2017 by researchers at Google.\n",
      "*   The model architecture has an encoder-decoder structure.\n",
      "\n",
      "The provided text does not explain what the encoder and decoder do, nor does it describe the other architectural features that are essential to understanding how a Transformer works.\n",
      "\n",
      "Source Documents:\n",
      "ðŸ“„ documents/LangChain.pdf\n",
      "ðŸ“ Wikimedia Commons):\n",
      "Figure 1.6: The Transformer architecture ...\n",
      "\n",
      "ðŸ“„ documents/LangChain.pdf\n",
      "ðŸ“ The architectural features that have contributed to the success of transformers are: ...\n",
      "\n",
      "ðŸ“„ documents/LangChain.pdf\n",
      "ðŸ“ The transformer model architecture has an encoder-decoder structure, where the encoder maps ...\n",
      "\n",
      "ðŸ“„ documents/LangChain.pdf\n",
      "ðŸ“ A transformer is a DL architecture, first introduced in 2017 by researchers at Google and the ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = self_rag_query(\"Explain Transformer architecture in detail\")\n",
    "\n",
    "print(\"\\nFinal Response:\\n\", response[\"result\"])\n",
    "\n",
    "if response[\"source_documents\"]:\n",
    "    print(\"\\nSource Documents:\")\n",
    "    for doc in response[\"source_documents\"]:\n",
    "        print(\"ðŸ“„\", doc.metadata.get(\"source\", \"Unknown source\"))\n",
    "        print(\"ðŸ“\", doc.page_content[:300], \"...\\n\")\n",
    "else:\n",
    "    print(\"\\nNo source documents used.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f226e10",
   "metadata": {},
   "source": [
    "### 2. Corrective RAG:\n",
    "#### - It identifies if the generated answer is wrong or misleading and correct it by fetching better sources\n",
    "![Corrective Rag](images/CRAG_working.png)\n",
    "![Corrective Rag](images/CRAG_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a0616d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def corrective_rag_query(query: str):\n",
    "    \"\"\"\n",
    "    Try answering with Gemini LLM. If unsure, fallback to RAG.\n",
    "    Always return a clean string answer.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ§  Query: {query}\")\n",
    "\n",
    "    # Step 1: Get initial Gemini LLM response\n",
    "    first_response = llm.invoke(query)\n",
    "\n",
    "    # Handle case where response is a list of messages\n",
    "    if isinstance(first_response, list):\n",
    "        first_response = first_response[0]\n",
    "\n",
    "    # Extract message content\n",
    "    try:\n",
    "        first_response_text = first_response.content.strip()\n",
    "    except AttributeError:\n",
    "        first_response_text = str(first_response).strip()\n",
    "\n",
    "    print(\"\\nðŸ”¸ Initial LLM Response:\\n\", first_response_text)\n",
    "\n",
    "    # Step 2: Check for weak response\n",
    "    if \"I don't know\" in first_response_text or \"I'm not sure\" in first_response_text or len(first_response_text) < 50:\n",
    "        print(\"\\nâš ï¸ Low confidence â€” using RAG for better result...\")\n",
    "\n",
    "        improved_response = qa_chain.invoke(query)\n",
    "        raw_answer = str(improved_response[\"result\"])\n",
    "\n",
    "        # Step 3: Clean up content=[...] format\n",
    "        match = re.search(r\"content=\\[(.*?)\\]\", raw_answer, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                content_list = eval(f\"[{match.group(1)}]\")\n",
    "                cleaned_answer = \" \".join(s.strip() for s in content_list)\n",
    "            except Exception as e:\n",
    "                cleaned_answer = \"âš ï¸ Error parsing content: \" + str(e)\n",
    "        else:\n",
    "            cleaned_answer = raw_answer.strip()\n",
    "\n",
    "        return {\"result\": cleaned_answer}\n",
    "    \n",
    "    # LLM confident â€” return its answer directly\n",
    "    return {\"result\": first_response_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afc81040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Query: Explain Transformer architecture in detail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\social_Eagle_python\\siva\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¸ Initial LLM Response:\n",
      " content=['Of course. Let\\'s break down the Transformer architecture in detail. It\\'s a powerful and complex model, so we\\'ll go step-by-step, from the big picture to the nitty-gritty components.\\n\\n### The Big Picture: What Problem Did Transformers Solve?\\n\\nBefore the Transformer, the state-of-the-art for sequence tasks (like machine translation) were **Recurrent Neural Networks (RNNs)**, including LSTMs and GRUs.\\n\\nRNNs had two major limitations:\\n1.  **Sequential Processing:** They process data one word at a time. This makes them slow and prevents parallelization on modern hardware (like GPUs).\\n2.  **Long-Range Dependencies:** While LSTMs improved this, it was still difficult for an RNN to connect a word at the end of a long paragraph to a word at the beginning. The \"memory\" would fade.\\n\\nThe Transformer, introduced in the 2017 paper **\"Attention Is All You Need,\"** solved these problems by completely removing recurrence and relying entirely on a mechanism called **self-attention**.\\n\\n---\\n\\n### High-Level Architecture: The Encoder-Decoder Stack\\n\\nAt its core, the Transformer is an **Encoder-Decoder** architecture, which is common in machine translation.\\n\\n*   **The Encoder:** Its job is to read the input sentence (e.g., \"The cat sat on the mat\") and build a rich, contextual representation of it. It understands the meaning and relationships within the input.\\n*   **The Decoder:** Its job is to take the Encoder\\'s representation and generate the output sentence one word at a time (e.g., \"Le chat s\\'est assis sur le tapis\").\\n\\nBoth the Encoder and Decoder are actually stacks of identical layers (the paper used N=6 for both).', '\\n\\nNow, let\\'s dive into the key components that make this work.\\n\\n---\\n\\n### The Deep Dive: Key Components\\n\\n#### 1. Input Embeddings & Positional Encoding\\n\\nComputers don\\'t understand words, they understand numbers.\\n*   **Input Embeddings:** Each word in the input sentence is converted into a vector (a list of numbers). This is done using a learned embedding matrix. Words with similar meanings will have similar vectors.\\n*   **Positional Encoding:** Since the model doesn\\'t have RNNs, it has no inherent sense of word order. \"The cat chased the dog\" is different from \"The dog chased the cat.\" To solve this, we inject information about the position of each word. The authors used a clever trick with sine and cosine functions of different frequencies.\\n    *   **Why sine and cosine?** They have properties that allow the model to easily learn relative positions. Each position gets a unique \"positional vector.\"\\n\\nThe final input vector for each word is the **sum of its embedding and its positional encoding**.\\n\\n#### 2. The Self-Attention Mechanism (The Star of the Show)\\n\\nThis is the core innovation. Self-attention allows the model to weigh the importance of different words in the input sequence when processing a specific word.\\n\\nImagine translating the sentence: \"The animal didn\\'t cross the street because **it** was too tired.\"\\n\\nWhen the model processes the word \"**it**,\" self-attention helps it figure out that \"**it**\" refers to \"**animal**\" and not \"**street**.\"\\n\\n**How it works (The Q, K, V Analogy):**\\n\\nThink of it like searching for a file in a database.\\n*   **Query (Q):** The word you\\'re currently focused on. It\\'s like your search query: \"What does \\'it\\' refer to?\"\\n*   **Key (K):** The \"labels\" or \"keywords\" of all the other words in the sentence. You compare your Query to these Keys to find a match.\\n*   **Value (V):** The actual content of the words. Once you find the best match (highest score between Q and K), you take its Value.\\n\\n**The Steps:**\\n\\n1.  **Create Q, K, V Vectors:** For each input word vector, we create three new vectorsâ€”Query, Key, and Valueâ€”by multiplying the input vector with three separate weight matrices (Wq, Wk, Wv). These matrices are learned during training.\\n2.  **Calculate Scores:** For the word we\\'re processing, we take its **Query** vector and calculate a dot product with the **Key** vectors of *all* other words in the sentence. This score represents how relevant each word is to our current word.\\n3.  **Scale:** The scores are divided by the square root of the dimension of the key vectors ($d_k$). This is a small but crucial step for stabilizing the gradients during training.\\n4.  **Softmax:** The scaled scores are passed through a softmax function. This turns the scores into probabilities that all sum to 1. The word with the highest score will have a high softmax value (e.g., \"animal\" might get 0.8), while irrelevant words will have tiny values (e.g., \"the\" might get 0.01).\\n5.  **Weighted Sum:** We multiply these softmax scores by the **Value** vectors of each word and sum them up. The result is a new vector for our current word that is a blend of all other words, heavily weighted by the ones that are most relevant.\\n\\nThis new vector is the output of the self-attention layer. It\\'s a representation of the word that is now rich with context from the entire sentence.', '\\n\\n#### 3. Multi-Head Attention\\n\\nInstead of doing self-attention just once, the Transformer does it multiple times in parallel. This is called **Multi-Head Attention**.\\n\\n*   **The Idea:** Each \"head\" can learn different types of relationships. For example, one head might focus on syntactic relationships (subject-verb), while another focuses on semantic relationships (what \"it\" refers to).\\n*   **How it works:** We split the Q, K, and V vectors into N smaller pieces (e.g., 8 \"heads\"). Each head performs the self-attention calculation in parallel. The resulting output vectors from all heads are then concatenated and passed through a final linear layer to produce the final output of the Multi-Head Attention block.\\n\\n#### 4. The \"Add & Norm\" Layer (Residual Connections)\\n\\nEach sub-layer in the Encoder and Decoder (like Multi-Head Attention or the Feed-Forward Network) has a residual connection around it, followed by layer normalization.\\n\\n*   **Add (Residual Connection):** The input to the sub-layer is added to the output of the sub-layer (`x + Sublayer(x)`). This is a powerful technique from computer vision (ResNets) that helps prevent gradients from vanishing in deep networks, making it easier to train many layers.\\n*   **Norm (Layer Normalization):** This standardizes the outputs of the layer to have a mean of 0 and a standard deviation of 1. It stabilizes and speeds up the training process.\\n\\n#### 5. Position-wise Feed-Forward Network\\n\\nAfter the attention layer in each encoder and decoder block, the output is passed through a simple fully-connected feed-forward network. This network is applied to each position (each word vector) separately and identically. It consists of two linear transformations with a ReLU activation in between.\\n\\nIts purpose is to add more computational depth and transform the attention outputs, allowing the model to learn more complex patterns. You can think of it as the \"thinking\" or \"processing\" part after the context has been gathered by the attention mechanism.\\n\\n---\\n\\n### Putting It All Together\\n\\n#### The Encoder Stack (N=6 layers)\\n\\n1.  Input sentence -> Embeddings + Positional Encoding.\\n2.  **Encoder Layer 1:**\\n    *   Multi-Head Self-Attention.\\n    *   Add & Norm.\\n    *   Feed-Forward Network.\\n    *   Add & Norm.\\n3.  The output of Layer 1 becomes the input for Layer 2.\\n4.  This repeats 6 times. The final output is a set of context-rich vectors (K and V) for each word in the input sentence. This output is then passed to the Decoder.\\n\\n#### The Decoder Stack (N=6 layers)\\n\\nThe Decoder is similar but has one crucial extra layer.\\n\\n1.  It takes the previously generated output sequence as its input (starting with a `<start>` token). This input also gets embedded and positionally encoded.\\n2.  **Decoder Layer 1:**\\n    *   **Masked Multi-Head Self-Attention:** This is self-attention on the output generated so far. The \"masking\" is critical: it prevents the model from \"cheating\" by looking at future words in the sequence it\\'s trying to predict. For position `i`, it can only attend to positions `1` to `i`.\\n    *   Add & Norm.\\n    *   **Encoder-Decoder Attention:** This is where the magic happens. The **Queries (Q)** come from the decoder\\'s masked attention layer above, but the **Keys (K) and Values (V)** come from the **final output of the Encoder stack**. This allows the decoder to look at the entire input sentence to decide what word to generate next.\\n    *   Add & Norm.\\n    *   Feed-Forward Network.\\n    *   Add & Norm.\\n3.  This repeats 6 times.\\n\\n#### The Final Output Layers\\n\\nAfter the final decoder block, the resulting vector is passed through:\\n1.  **A Linear Layer:** A fully connected layer that projects the vector into a much larger vector, called a logits vector. The size of this vector is the size of our entire vocabulary.\\n2.  **A Softmax Layer:** This turns the logits into probabilities. Each element in the vector corresponds to a word in the vocabulary, and its value is the probability that this word is the next word in the sequence. The word with the highest probability is chosen as the output.\\n\\nThis process repeats, feeding the newly generated word back into the decoder, until a special `<end_of_sentence>` token is produced.\\n\\n### Summary: Why the Transformer is a Game-Changer\\n\\n1.  **Parallelization:** By removing recurrence, the entire sequence can be processed at once, making it dramatically faster to train on modern hardware.\\n2.  **Direct Long-Range Dependencies:** The attention mechanism directly connects every word to every other word, with a path length of just 1. This makes it exceptionally good at capturing long-range context.\\n3.  **Foundation for Modern AI:** The Transformer architecture is the foundation for almost all modern large language models (LLMs), including BERT (which uses only the Encoder) and the GPT family (which uses only the Decoder).'] additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--a173a373-f6f9-49a9-a17b-857febded00e-0' usage_metadata={'input_tokens': 6, 'output_tokens': 2299, 'total_tokens': 3914, 'input_token_details': {'cache_read': 0}}\n",
      "\n",
      "âœ… Final Answer:\n",
      "\n",
      "Of course. Let's break down the Transformer architecture in detail. It's a powerful and complex model, so we'll go step-by-step, from the big picture to the nitty-gritty components.\n",
      "\n",
      "### The Big Picture: What Problem Did Transformers Solve?\n",
      "\n",
      "Before the Transformer, the state-of-the-art for sequence tasks (like machine translation) were **Recurrent Neural Networks (RNNs)**, including LSTMs and GRUs.\n",
      "\n",
      "RNNs had two major limitations:\n",
      "1.  **Sequential Processing:** They process data one word at a time. This makes them slow and prevents parallelization on modern hardware (like GPUs).\n",
      "2.  **Long-Range Dependencies:** While LSTMs improved this, it was still difficult for an RNN to connect a word at the end of a long paragraph to a word at the beginning. The \"memory\" would fade.\n",
      "\n",
      "The Transformer, introduced in the 2017 paper **\"Attention Is All You Need,\"** solved these problems by completely removing recurrence and relying entirely on a mechanism called **self-attention**.\n",
      "\n",
      "---\n",
      "\n",
      "### High-Level Architecture: The Encoder-Decoder Stack\n",
      "\n",
      "At its core, the Transformer is an **Encoder-Decoder** architecture, which is common in machine translation.\n",
      "\n",
      "*   **The Encoder:** Its job is to read the input sentence (e.g., \"The cat sat on the mat\") and build a rich, contextual representation of it. It understands the meaning and relationships within the input.\n",
      "*   **The Decoder:** Its job is to take the Encoder's representation and generate the output sentence one word at a time (e.g., \"Le chat s'est assis sur le tapis\").\n",
      "\n",
      "Both the Encoder and Decoder are actually stacks of identical layers (the paper used N=6 for both). Now, let's dive into the key components that make this work.\n",
      "\n",
      "---\n",
      "\n",
      "### The Deep Dive: Key Components\n",
      "\n",
      "#### 1. Input Embeddings & Positional Encoding\n",
      "\n",
      "Computers don't understand words, they understand numbers.\n",
      "*   **Input Embeddings:** Each word in the input sentence is converted into a vector (a list of numbers). This is done using a learned embedding matrix. Words with similar meanings will have similar vectors.\n",
      "*   **Positional Encoding:** Since the model doesn't have RNNs, it has no inherent sense of word order. \"The cat chased the dog\" is different from \"The dog chased the cat.\" To solve this, we inject information about the position of each word. The authors used a clever trick with sine and cosine functions of different frequencies.\n",
      "    *   **Why sine and cosine?** They have properties that allow the model to easily learn relative positions. Each position gets a unique \"positional vector.\"\n",
      "\n",
      "The final input vector for each word is the **sum of its embedding and its positional encoding**.\n",
      "\n",
      "#### 2. The Self-Attention Mechanism (The Star of the Show)\n",
      "\n",
      "This is the core innovation. Self-attention allows the model to weigh the importance of different words in the input sequence when processing a specific word.\n",
      "\n",
      "Imagine translating the sentence: \"The animal didn't cross the street because **it** was too tired.\"\n",
      "\n",
      "When the model processes the word \"**it**,\" self-attention helps it figure out that \"**it**\" refers to \"**animal**\" and not \"**street**.\"\n",
      "\n",
      "**How it works (The Q, K, V Analogy):**\n",
      "\n",
      "Think of it like searching for a file in a database.\n",
      "*   **Query (Q):** The word you're currently focused on. It's like your search query: \"What does 'it' refer to?\"\n",
      "*   **Key (K):** The \"labels\" or \"keywords\" of all the other words in the sentence. You compare your Query to these Keys to find a match.\n",
      "*   **Value (V):** The actual content of the words. Once you find the best match (highest score between Q and K), you take its Value.\n",
      "\n",
      "**The Steps:**\n",
      "\n",
      "1.  **Create Q, K, V Vectors:** For each input word vector, we create three new vectorsâ€”Query, Key, and Valueâ€”by multiplying the input vector with three separate weight matrices (Wq, Wk, Wv). These matrices are learned during training.\n",
      "2.  **Calculate Scores:** For the word we're processing, we take its **Query** vector and calculate a dot product with the **Key** vectors of *all* other words in the sentence. This score represents how relevant each word is to our current word.\n",
      "3.  **Scale:** The scores are divided by the square root of the dimension of the key vectors ($d_k$). This is a small but crucial step for stabilizing the gradients during training.\n",
      "4.  **Softmax:** The scaled scores are passed through a softmax function. This turns the scores into probabilities that all sum to 1. The word with the highest score will have a high softmax value (e.g., \"animal\" might get 0.8), while irrelevant words will have tiny values (e.g., \"the\" might get 0.01).\n",
      "5.  **Weighted Sum:** We multiply these softmax scores by the **Value** vectors of each word and sum them up. The result is a new vector for our current word that is a blend of all other words, heavily weighted by the ones that are most relevant.\n",
      "\n",
      "This new vector is the output of the self-attention layer. It's a representation of the word that is now rich with context from the entire sentence. #### 3. Multi-Head Attention\n",
      "\n",
      "Instead of doing self-attention just once, the Transformer does it multiple times in parallel. This is called **Multi-Head Attention**.\n",
      "\n",
      "*   **The Idea:** Each \"head\" can learn different types of relationships. For example, one head might focus on syntactic relationships (subject-verb), while another focuses on semantic relationships (what \"it\" refers to).\n",
      "*   **How it works:** We split the Q, K, and V vectors into N smaller pieces (e.g., 8 \"heads\"). Each head performs the self-attention calculation in parallel. The resulting output vectors from all heads are then concatenated and passed through a final linear layer to produce the final output of the Multi-Head Attention block.\n",
      "\n",
      "#### 4. The \"Add & Norm\" Layer (Residual Connections)\n",
      "\n",
      "Each sub-layer in the Encoder and Decoder (like Multi-Head Attention or the Feed-Forward Network) has a residual connection around it, followed by layer normalization.\n",
      "\n",
      "*   **Add (Residual Connection):** The input to the sub-layer is added to the output of the sub-layer (`x + Sublayer(x)`). This is a powerful technique from computer vision (ResNets) that helps prevent gradients from vanishing in deep networks, making it easier to train many layers.\n",
      "*   **Norm (Layer Normalization):** This standardizes the outputs of the layer to have a mean of 0 and a standard deviation of 1. It stabilizes and speeds up the training process.\n",
      "\n",
      "#### 5. Position-wise Feed-Forward Network\n",
      "\n",
      "After the attention layer in each encoder and decoder block, the output is passed through a simple fully-connected feed-forward network. This network is applied to each position (each word vector) separately and identically. It consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "Its purpose is to add more computational depth and transform the attention outputs, allowing the model to learn more complex patterns. You can think of it as the \"thinking\" or \"processing\" part after the context has been gathered by the attention mechanism.\n",
      "\n",
      "---\n",
      "\n",
      "### Putting It All Together\n",
      "\n",
      "#### The Encoder Stack (N=6 layers)\n",
      "\n",
      "1.  Input sentence -> Embeddings + Positional Encoding.\n",
      "2.  **Encoder Layer 1:**\n",
      "    *   Multi-Head Self-Attention.\n",
      "    *   Add & Norm.\n",
      "    *   Feed-Forward Network.\n",
      "    *   Add & Norm.\n",
      "3.  The output of Layer 1 becomes the input for Layer 2.\n",
      "4.  This repeats 6 times. The final output is a set of context-rich vectors (K and V) for each word in the input sentence. This output is then passed to the Decoder.\n",
      "\n",
      "#### The Decoder Stack (N=6 layers)\n",
      "\n",
      "The Decoder is similar but has one crucial extra layer.\n",
      "\n",
      "1.  It takes the previously generated output sequence as its input (starting with a `<start>` token). This input also gets embedded and positionally encoded.\n",
      "2.  **Decoder Layer 1:**\n",
      "    *   **Masked Multi-Head Self-Attention:** This is self-attention on the output generated so far. The \"masking\" is critical: it prevents the model from \"cheating\" by looking at future words in the sequence it's trying to predict. For position `i`, it can only attend to positions `1` to `i`.\n",
      "    *   Add & Norm.\n",
      "    *   **Encoder-Decoder Attention:** This is where the magic happens. The **Queries (Q)** come from the decoder's masked attention layer above, but the **Keys (K) and Values (V)** come from the **final output of the Encoder stack**. This allows the decoder to look at the entire input sentence to decide what word to generate next.\n",
      "    *   Add & Norm.\n",
      "    *   Feed-Forward Network.\n",
      "    *   Add & Norm.\n",
      "3.  This repeats 6 times.\n",
      "\n",
      "#### The Final Output Layers\n",
      "\n",
      "After the final decoder block, the resulting vector is passed through:\n",
      "1.  **A Linear Layer:** A fully connected layer that projects the vector into a much larger vector, called a logits vector. The size of this vector is the size of our entire vocabulary.\n",
      "2.  **A Softmax Layer:** This turns the logits into probabilities. Each element in the vector corresponds to a word in the vocabulary, and its value is the probability that this word is the next word in the sequence. The word with the highest probability is chosen as the output.\n",
      "\n",
      "This process repeats, feeding the newly generated word back into the decoder, until a special `<end_of_sentence>` token is produced.\n",
      "\n",
      "### Summary: Why the Transformer is a Game-Changer\n",
      "\n",
      "1.  **Parallelization:** By removing recurrence, the entire sequence can be processed at once, making it dramatically faster to train on modern hardware.\n",
      "2.  **Direct Long-Range Dependencies:** The attention mechanism directly connects every word to every other word, with a path length of just 1. This makes it exceptionally good at capturing long-range context.\n",
      "3.  **Foundation for Modern AI:** The Transformer architecture is the foundation for almost all modern large language models (LLMs), including BERT (which uses only the Encoder) and the GPT family (which uses only the Decoder).\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain Transformer architecture in detail\"\n",
    "response = corrective_rag_query(query)\n",
    "\n",
    "# Raw string (your response)\n",
    "raw_answer = str(response[\"result\"])  # Already a string based on your output\n",
    "\n",
    "# Step 1: Extract the part inside content=[ ... ]\n",
    "match = re.search(r\"content=\\[(.*?)\\]\", raw_answer, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    # Step 2: Join the parts into one string, removing any leftover quote marks\n",
    "    content_list = eval(f\"[{match.group(1)}]\")  # safely convert the inner list\n",
    "    cleaned_answer = \" \".join(s.strip() for s in content_list)\n",
    "else:\n",
    "    # fallback if pattern not found\n",
    "    cleaned_answer = raw_answer\n",
    "\n",
    "# Output result\n",
    "print(\"\\nâœ… Final Answer:\\n\")\n",
    "print(cleaned_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d6526",
   "metadata": {},
   "source": [
    "### 3. Fusion Rag :\n",
    "#### - It retrievers multiple documents and merges their information to generate a comprehensive, non-redundant answer.\n",
    "![Fusion RaG](images/Fusion_rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0012fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\social_Eagle_python\\siva\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_pinecone import Pinecone as LangchainPinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af8a2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sivaa\\AppData\\Local\\Temp\\ipykernel_3404\\2858869753.py:7: LangChainDeprecationWarning: The class `Pinecone` was deprecated in LangChain 0.0.3 and will be removed in 1.0.0. Use :class:`~PineconeVectorStore` instead.\n",
      "  vectorstore = LangchainPinecone(\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize the Gemini Embedding Model\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\"  # or use 'text-embedding-3-large' if needed\n",
    ")\n",
    "\n",
    "# Step 3: Connect to existing Pinecone vector store\n",
    "vectorstore = LangchainPinecone(\n",
    "    embedding=embedding_model,\n",
    "    index_name=\"langchainpdf\",  # âœ… Your existing index name\n",
    "    namespace=\"llmpdf\"          # âœ… The namespace you used earlier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "677bdd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 4: Initialize Gemini 2.5 Pro LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0.3,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# âœ… Step 5: Connect LLM with your existing retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,  # Use your Pinecone or FAISS retriever here\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\"  # Use \"stuff\" for simple concatenation of documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7017500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sivaa\\AppData\\Local\\Temp\\ipykernel_3404\\2115752122.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n",
      "d:\\program\\python\\social_Eagle_python\\siva\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Answer:\n",
      "I cannot provide a summary because the text of the PDF was not included in the context. The provided text is a set of instructions and examples about how to summarize a document, not the content of the document itself.\n",
      "\n",
      "ðŸ“„ Source Documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Write a concise summary of the following:\n",
      "{text}\n",
      "CONCISE SUMMARY: ...\n",
      "\n",
      "--- Document 2 ---\n",
      "their outputs.\n",
      "Hereâ€™s a simple example of loading a PDF document and summarizing it: ...\n",
      "\n",
      "--- Document 3 ---\n",
      "in a more concise and simplified manner. It can also answer specific questions about the paper, ...\n",
      "\n",
      "--- Document 4 ---\n",
      "look at summarization in much more detail in Chapter 4, Building Capable Assistants. Letâ€™s move on. ...\n"
     ]
    }
   ],
   "source": [
    "# âœ… Step 6: Perform a query\n",
    "query = \"What is the summary of this PDF?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Show answer\n",
    "print(\"ðŸ“Œ Answer:\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "# Show sources\n",
    "print(\"\\nðŸ“„ Source Documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(doc.page_content[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb44aa",
   "metadata": {},
   "source": [
    "### Advance RAG :\n",
    "#### - It uses the external tools, multi-step reasoning or planing agents to complex queries.\n",
    "#### - It can retriever, process and combine information on in stages.\n",
    "\n",
    "![Advance_Rag](images/advance_RAG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe5b0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\n",
    "    \"k\": 10,\n",
    "    \"score_threshold\": 0.7  # filters weak matches\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b978486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use map_reduce for summarization + reasoning\n",
    "qa_chain_arag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"map_reduce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed56736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\social_Eagle_python\\siva\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Answer:\n",
      "Based on the provided text, the key transformer architecture concept explained is that it has an **encoder-decoder structure**. The text also begins to explain that the encoder's function is to map an input, but the sentence is cut off.\n",
      "\n",
      "ðŸ“„ Sources:\n",
      "\n",
      "--- Source 1 (page 43.0) ---\n",
      "The architectural features that have contributed to the success of transformers are: ...\n",
      "\n",
      "--- Source 2 (page 43.0) ---\n",
      "Wikimedia Commons):\n",
      "Figure 1.6: The Transformer architecture ...\n",
      "\n",
      "--- Source 3 (page 57.0) ---\n",
      "7. What is a transformer and what does it consist of?\n",
      "8. What does GPT stand for? ...\n",
      "\n",
      "--- Source 4 (page 42.0) ---\n",
      "The transformer model architecture has an encoder-decoder structure, where the encoder maps ...\n"
     ]
    }
   ],
   "source": [
    "query = \"Summarize the key transformer architecture concepts explained in this PDF.\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"ðŸ“Œ Answer:\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "print(\"\\nðŸ“„ Sources:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\n--- Source {i+1} (page {doc.metadata.get('page', '?')}) ---\")\n",
    "    print(doc.page_content[:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823037b",
   "metadata": {},
   "source": [
    "### Speculative RAG :\n",
    "#### - Instead of retrieying first, speculative rag start by guessing answer based on query, then retrievers document to refine or verity\n",
    "#### - Helps when intial retrieval is weak.\n",
    "#### flow :\n",
    "#### Input -> Guess topic -> vector DB -> documents -> LLM -> Answer\n",
    "\n",
    "![Speculative_RAG](images/speculative_rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1e1d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce757323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft (fast) and Target (accurate) can be same if you donâ€™t have GPT-4 etc.\n",
    "draft_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", google_api_key=GOOGLE_API_KEY)\n",
    "target_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cee64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_rag(query: str, retriever, draft_llm, final_llm):\n",
    "    # 1. Retrieve context\n",
    "    context_docs = retriever.get_relevant_documents(query)\n",
    "    context_text = \"\\n---\\n\".join([doc.page_content for doc in context_docs])\n",
    "\n",
    "    # 2. Draft answer from fast LLM\n",
    "    draft_prompt = f\"\"\"Context:\\n{context_text}\\n\\nQuestion:\\n{query}\\n\\nDraft Answer:\"\"\"\n",
    "    draft_answer = draft_llm.invoke(draft_prompt)\n",
    "\n",
    "    # 3. Refine using final LLM\n",
    "    refine_prompt = f\"\"\"You're a helpful assistant. Please refine the following draft based on context.\\n\\nDraft:\\n\\\"\\\"\\\"\\n{draft_answer}\\n\\\"\\\"\\\"\\n\\nRefined Answer:\"\"\"\n",
    "    refined_answer = final_llm.invoke(refine_prompt)\n",
    "\n",
    "    return refined_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04c70773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6acf8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ðŸ¤– Final Answer\n",
       "\n",
       "Of course. Here is a refined version of the draft, edited for clarity, flow, and impact.\n",
       "\n",
       "***\n",
       "\n",
       "### **Key Principles of the Transformer Architecture**\n",
       "\n",
       "The transformer model revolutionized how machines process sequential data like text, largely replacing traditional recurrent architectures. Its power and efficiency are built on the following core principles:\n",
       "\n",
       "1.  **Self-Attention Mechanism**\n",
       "    At the heart of the transformer is the **self-attention mechanism**. Unlike recurrent models that process words one by one, self-attention allows the model to weigh the importance of all other words in a sequence when encoding a specific word. This enables it to capture complex, **long-range dependencies** and contextual relationships, regardless of their distance from each other. Crucially, these calculations can be performed for all tokens simultaneously, making the process highly **parallelizable** and significantly speeding up training.\n",
       "\n",
       "2.  **Multi-Head Attention**\n",
       "    Instead of calculating attention just once, the model employs **multi-head attention**. This means it runs the self-attention process multiple times in parallel. Each \"head\" can learn different types of contextual relationshipsâ€”for instance, one head might focus on syntactic links while another tracks semantic connections. The outputs from all heads are then combined to create a richer, more nuanced representation of the input.\n",
       "\n",
       "3.  **Positional Encodings**\n",
       "    Because the self-attention mechanism is inherently order-agnostic, the model needs a way to understand the sequence of the input. This is solved by adding **positional encodings** to the input embeddings. These are vectors that provide information about the absolute or relative position of each token, ensuring that the model can factor word order into its calculations.\n",
       "\n",
       "4.  **Encoder-Decoder Architecture**\n",
       "    The original transformer model featured a two-part structure:\n",
       "    *   **The Encoder:** A stack of layers that processes the entire input sequence at once to build a rich, contextualized representation. Each encoder layer contains a multi-head self-attention mechanism and a feed-forward network.\n",
       "    *   **The Decoder:** A stack of layers that generates the output sequence one token at a time. In addition to the components found in the encoder, the decoder includes a third sub-layer that performs \"encoder-decoder attention,\" allowing it to focus on the most relevant parts of the encoded input.\n",
       "\n",
       "    *(Note: This two-part structure is characteristic of the original model. Many modern architectures use only one part, such as encoder-only models (e.g., BERT) or decoder-only models (e.g., GPT).)*\n",
       "\n",
       "5.  **Feed-Forward Networks & Layer Normalization**\n",
       "    To support the attention mechanisms, each layer also contains two key components:\n",
       "    *   A simple, position-wise **feed-forward network** that adds computational depth and non-linearity, further processing the output of the attention layer.\n",
       "    *   **Residual connections** and **layer normalization** are applied around each sub-layer. These techniques are critical for stabilizing the training of very deep networks by preventing gradients from vanishing or exploding."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What are the key principles behind the transformer model?\"\n",
    "\n",
    "refined = speculative_rag(\n",
    "    query=query,\n",
    "    retriever=retriever,\n",
    "    draft_llm=draft_llm,\n",
    "    final_llm=target_llm\n",
    ")\n",
    "\n",
    "refined_text = refined.content\n",
    "\n",
    "markdown_ready = f\"## ðŸ¤– Final Answer\\n\\n{refined_text}\"\n",
    "\n",
    "display(Markdown(markdown_ready))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636e645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
